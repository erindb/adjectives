---
title: "Adjectives"
author: "erin"
output:
  pdf_document:
    toc: true
    highlight: zenburn
---

```{r, echo=FALSE}
# install dependencies
library(ggplot2, quiet=T)
library(plyr, quiet=T)

marginalize = function(df) {
  value_df = ddply(df,
             .(alternatives, prior_distribution, cost_of_adjective,
               utterance, speaker_lambda, value),
             summarize,
             probability = sum(probability))
  theta_df = ddply(df,
             .(alternatives, prior_distribution, cost_of_adjective,
               utterance, speaker_lambda, theta),
             summarize,
             probability = sum(probability))
  value_df$variable = "value"
  theta_df$variable = "theta"
  names(theta_df)[names(theta_df) == "theta"] = "value"
  value_df$variable = factor(value_df$variable, levels=c("value", "theta"))
  theta_df$variable = factor(theta_df$variable, levels=c("value", "theta"))
  new_df = rbind(value_df, theta_df)
  return(new_df)
}
```

# Math

Literal Listener's probability distribution over the values $X$ are is prior, conditioned on the utterance being true and renormalized.

$$P_{L0}(x | u, \theta) \propto \delta_{u \mbox{ is true}} \cdot P(x)$$

Speaker's utility is the negative cost and the log probability of the actual state of the world under the Literal Listener's posterior. This means that the more surprised the Literal Listener would be to hear the true state of the world after already hearing the utterance, the less good the utterance would be.

$$\mathbb{U}_{S}(u | x, \theta) = log(P_{L0}(x|u, \theta)) - cost(u)$$

The speaker then chooses an utterance by soft-maximizing their utility funciton.

$$P_{S}(u|x, \theta) \propto e^{\lambda \mathbb{U}_{S}(u|x, \theta)}$$

The pragmatic listener infers both the threshold $\theta$ and the value $x$ conditioning on the speaker choosing the given utterance.

$$P_{L1}(x, \theta | u) \propto P_{S}(u|x, \theta)P(x)P(\theta)$$

## Questions

* What is the expected threshold for different distributions?
    - Is there a constant percent of the prior probability mass that this expected threshold corresponds to (for a lot of contexts?)?

## Dimensions of variation

* What alternative utterances are possible?
    * say nothing
    * say adjective (e.g. "tall")
    * negate adjective (e.g. "not tall")
    * say opposite of adjective (e.g. "short")
    * negate opposite of adjective (e.g. "not short")
    * communicate exactly (all possible utterances that would communicate exactly the speaker's state of mind -- could be infinitely costly)
* What are the costs of the alternative utterances?
* Different prior distributions along the adjective scale.
* Is the threshold $\theta$ lifted all the way to the pragmatic listener? Or inferred by the literal listener?
* What is the rationality parameter of the speaker? of the pragmatic listener? Might the speaker's rationality be inferred?

# Different input priors

## Continuous, bounded, "toy" priors

I looked at several different prior distributions on the bounded interval [0,1]:

* (bounded) normal: $\delta_{[0,1]} \cdot \mathcal{N}(0.5, 0.1)$
* uniform: $\mathcal{U}(0, 1)$
* betas:
    * $\mbox{Beta}(1, 5)$
    * $\mbox{Beta}(5, 1)$
* (bounded) log-normal:
    * $\delta_{[0,1]} \cdot \ln \mathcal{N}(-1, 0.3)$
    * reversed: $\delta_{[0,1]} \cdot \ln \mathcal{N}(1-x ; -1, 0.3)$
* linear:
    * $2-2x$
    * $2x$

These distributions were discretized for the simulations.

```{r, echo=FALSE, fig.width=7, fig.height=3}
x = seq(from=0, to=1, by=0.01)
dists = list(
  list("normal", dnorm(x, mean=0.5, sd=0.1))
  , list("uniform", dunif(x, 0, 1))
  , list("beta(1, 5)", dbeta(x, 1, 5))
  , list("beta(5, 1)", dbeta(x, 5, 1))
  , list("lognormal(-1, 0.3)", dlnorm(x, meanlog=-1, sdlog=0.3))
  , list("lognormal(1-x, -1, 0.3)", dlnorm(1-x, meanlog=-1, sdlog=0.3))
  , list("2-2x", 2-2*x)
  , list("2x", 2*x)
)
d = data.frame(x=c(rep(x, length(dists))),
               y=unlist(sapply(dists, function(lst) {return(lst[2])})),
               dist=c(sapply(dists, function(lst) {return(rep(lst[[1]], length(x)))})))

### graph distributions:
p = ggplot(d, aes(x=x, y=y, colour=dist, fill=dist)) +
  geom_line(lwd=1) +
  theme_bw(12) +
  theme(panel.grid=element_blank()) +
  ggtitle("Various Bounded Continuous Priors")
print(p)

# ## to print prior distributions for webppl input:
# dist_data = data.frame(
#   type=c(rep("x", length(x)),
#          unlist(sapply(dists, function(dist) {return(rep(dist[[1]], length(x)))}))),
#   probability=c(x,
#                 sapply(dists, function(dist) {return(dist[[2]])}))
# )
# write.table(dist_data, sep=",", row.names=F, col.names=F, file="data/input/toy_priors.csv")
```

## Real priors from Anthea's experiments

```{r, echo=F, fig.width=8.5, fig.height=6}
anthea_humans_wide = read.table("data/input/sliderMeans3.csv", header=T, sep=",")
anthea_humans = reshape(anthea_humans_wide, direction="long", varying=4:18, sep="", timevar="bin")
p = ggplot(subset(anthea_humans, condition != "few"), aes(x=bin, y=sliderNorm, colour=condition)) +
  geom_line() +
  facet_wrap(~ item, scale="free") +
  ggtitle("Anthea's Priors and Posteriors (after 'Many') for Various Contexts") +
  theme_bw(10) +
  theme(panel.grid=element_blank())
print(p)

# ## to print prior distributions for webppl input:
# dist_data = anthea_humans[anthea_humans$condition == "none", c("item", "sliderNorm")]
# dist_data = rbind(dist_data, data.frame(item=rep("x", 15), sliderNorm=1:15))
# write.table(dist_data, sep=",", row.names=F, col.names=F, file="data/input/anthea_priors.csv")
```

# Simulations

## Code

Here's a sketch of a scalar adjectives model in WebPPL.

```
var literal_listener = cache(function(utterance, theta) {
	Enumerate(function() {
		var value = value_prior();
		factor(meaning(utterance, theta, value) ? 0 : -Infinity);
		return value;
	})
})

var speaker = cache(function(value, theta) {
	Enumerate(function() {
		var utterance = utterance_prior();
		var literal_interpretation = literal_listener(utterance, theta);
		factor(literal_interpretation.score([], value));
		return utterance;
	})
})

var listener = function(utterance) {
	var value = value_prior();
	var theta = theta_prior();
	var speaker_choice = speaker(value, theta);
	factor(speaker_choice.score([], utterance));
	return [value, theta];
}
```

## Results

### On continuous, bounded, "toy" priors

#### Posteriors

```{r, echo=F, fig.width=8.5, fig.height=4}
### need to modify this
webppl_output = rbind(
  read.table("data/output/toy_priors/normal.csv", sep=",", header=T)
  , read.table("data/output/toy_priors/uniform.csv", sep=",", header=T)
  , read.table("data/output/toy_priors/beta1_5.csv", sep=",", header=T)
  , read.table("data/output/toy_priors/beta5_1.csv", sep=",", header=T)
  , read.table("data/output/toy_priors/lognormal1-x_-1_0.3.csv", sep=",", header=T)
  , read.table("data/output/toy_priors/lognormal-1_0.3.csv", sep=",", header=T)
  , read.table("data/output/toy_priors/2x.csv", sep=",", header=T)
  , read.table("data/output/toy_priors/2-2x.csv", sep=",", header=T)
)
webppl_output$prior_distribution = factor(webppl_output$prior_distribution,
                                            levels=c(
                                              "2-2x",
                                              "2x",
                                              "beta0.1_0.1",
                                              "beta1_5",
                                              "beta5_1",
                                              "lognormal-1_0.3",
                                              "lognormal1-x_-1_0.3",
                                              "normal",
                                              "uniform"
                                            ), labels=c(
                                              "2-2x",
                                              "2x",
                                              "beta(0.1,0.1)",
                                              "beta(1, 5)",
                                              "beta(5, 1)",
                                              "lognormal(-1, 0.3)",
                                              "lognormal(1-x, -1, 0.3)",
                                              "normal",
                                              "uniform"
                                            ))
webppl_output$probability = as.numeric(as.character(webppl_output$probability))

marginalized_output = marginalize(webppl_output)

theta_output = subset(marginalized_output, (utterance == "prior" & variable == "value") |
                        (utterance == "adjective" & variable == "theta"))
theta_output$group = "prior"
theta_output$group[theta_output$variable == "theta"] = "posterior_theta"

p = ggplot(theta_output, aes(x=value, y=probability, linetype=group,
                             colour=prior_distribution)) +
  geom_line(lwd=0.7) +
  facet_grid(cost_of_adjective ~ speaker_lambda, scale="free") +
  theme_bw(10) +
  theme(panel.grid=element_blank()) +
  ggtitle("WebPPL output for theta posterior with various parameters")
print(p)
```

#### Expected inferred thresholds:

Expected thresholds $\theta$ for different distributions as both values and percentiles of the prior distribution along the adjective scale. For these results, the rationality parameter for the speaker, $\lambda$, was 5 and the cost of the adjective utterance was 6. The possible utterances for the speaker were: say nothing, say the adjective, and negate the adjective (same threshold $\theta$, but opposite direction).

```{r, echo=FALSE, fig.width=7, fig.height=3}

expected_thetas = ddply(subset(theta_output, variable == "theta" &
                                 cost_of_adjective == 1 &
                                 speaker_lambda == 1),
                        .(prior_distribution),
                        summarize,
                        expected = sum( value * probability)) #get mode later
actual_priors = subset(marginalized_output, utterance == "prior" &
                  cost_of_adjective == 1 &
                  speaker_lambda == 1 &
                  variable == "value")
actual_priors = actual_priors[,c("prior_distribution", "value", "probability")]
expected_thetas$percentile = sapply(1:nrow(expected_thetas), function(i) {
  prior_dist = as.character(expected_thetas$prior_distribution)[[i]]
  expected = expected_thetas$expected[[i]]
  subd = subset(actual_priors, prior_distribution == prior_dist & value <= expected)
  return(sum(subd$probability))
})
print(expected_thetas)
```

These values are not all the same.

### On Anthea's elicited priors

* I might want to fit the parameters to the adjective posterior, but first we can try alpha5 cost6.
* Graph model posterior against human posterior.
* Get expected thetas and compare to Anthea's exptected thetas.

Here's the output of our lifted variable model compared to Anthea's data for "many".

```{r, echo=F, fig.width=8.5, fig.height=7}
files <- dir("data/output/anthea_priors/", pattern="*.csv")
webppl_anthea_output = data.frame()
for (file in files) {
  webppl_anthea_output = rbind(
    webppl_anthea_output,
    read.table(paste("data/output/anthea_priors/", file, sep=""), sep=",", header=T)
  )
}
marginalized_anthea_output = marginalize(webppl_anthea_output)
anthea_humans$prior_distribution = anthea_humans$item

model = subset(marginalized_anthea_output, !(prior_distribution %in% c(
#   "cig", "move"
  )))
human = subset(anthea_humans, !(prior_distribution %in% c(
#   "cig", "move"
  )))

p = ggplot(data=subset(model, utterance == "adjective" & variable == "value"),
           aes(x=value, y=probability)) +
  geom_line(aes(colour=ordered(cost_of_adjective), linetype=ordered(speaker_lambda))) +
  geom_line(data=subset(human, condition == "many"), aes(x=bin, y=sliderNorm), colour="black") +
  facet_wrap(~ prior_distribution, scale="free") +
  ggtitle("Posterior distributions for humans versus model with various parameter values") +
  theme_bw(10) +
  theme(panel.grid=element_blank())
print(p)
```

```{r, echo=F}
### for each prior distribution and parameter combo, get a KLD
model_klds = ddply(.data=subset(model, utterance == "adjective" & variable == "value"), .(speaker_lambda, cost_of_adjective, prior_distribution), .fun=function(df) {
  human_subd = subset(human, prior_distribution == df$prior_distribution[[1]] & condition == "many")
  kld = sum( df$probability * log(df$probability / human_subd$sliderNorm) )
  newdf = df[1,]
  newdf$kld = kld
  newdf$value = NULL
  newdf$probability = NULL
  return(newdf)
})

### for each parameter combo, get average KLD across distributions
average_klds = ddply(.data=model_klds, .(speaker_lambda, cost_of_adjective), summarize, kld = mean(kld))
best_parameters = average_klds[average_klds$kld == min(average_klds$kld),]

best_speaker_lambda=best_parameters$speaker_lambda[[1]]
best_cost_of_adjective=best_parameters$cost_of_adjective[[1]]
```

The best fit parameters (i.e. with the lowest average KL divergence) are speaker rationality $\lambda =$ `r best_speaker_lambda` and cost of utterance `r best_cost_of_adjective`.

```{r, echo=F, fig.width=8.5, fig.height=7}
model_vs_human = subset(model, utterance == "adjective" & variable == "value" &
                         cost_of_adjective == best_cost_of_adjective &
                         speaker_lambda == best_speaker_lambda)[, c("prior_distribution", "value", "probability")]
model_vs_human$model_or_human = "model"
human_posterior = subset(human, condition == "many")
model_vs_human = rbind(model_vs_human, data.frame(
  prior_distribution = human_posterior$item,
  value = human_posterior$bin,
  probability = human_posterior$sliderNorm,
  model_or_human = rep("human", nrow(human_posterior))
))

model_vs_human$model_or_human = factor(model_vs_human$model_or_human,
                                       levels=c("model", "human"))
p = ggplot(data=model_vs_human, aes(x = value, y= probability, colour=prior_distribution, linetype=model_or_human)) +
  geom_line() +
  facet_wrap(~ prior_distribution, scale="free") +
  ggtitle("Posterior distributions for humans versus model with best parameter values") +
  theme_bw(10) +
  theme(panel.grid=element_blank())
print(p)
```

```{r, echo=F}
expected_thetas = ddply(subset(model, utterance == "adjective" & variable == "theta" &
                         cost_of_adjective == best_cost_of_adjective &
                         speaker_lambda == best_speaker_lambda),
                        .(prior_distribution),
                        summarize,
                        expected = sum( value * probability)) #get mode later
actual_priors = subset(marginalized_anthea_output, utterance == "prior" &
                  cost_of_adjective == 1 &
                  speaker_lambda == 1 &
                  variable == "value")
actual_priors = actual_priors[,c("prior_distribution", "value", "probability")]
expected_thetas$percentile = sapply(1:nrow(expected_thetas), function(i) {
  prior_dist = as.character(expected_thetas$prior_distribution)[[i]]
  expected = expected_thetas$expected[[i]]
  subd = subset(actual_priors, prior_distribution == prior_dist & value <= expected)
  return(sum(subd$probability))
})

best_KLs = subset(model_klds, cost_of_adjective==best_cost_of_adjective & speaker_lambda == best_speaker_lambda)
stopifnot(best_KLs$prior_distribution == expected_thetas$prior_distribution)
expected_thetas$KL = best_KLs$kld
print(expected_thetas)

### percentiles that are more different don't correspond to higher KLs.
### (if they did, that might be evidence for a single theta value)
# ggplot(data=expected_thetas, aes(x=KL, y=percentile)) +
#   geom_point() +
#   geom_smooth(method="lm") +
#   xlab("KL divergence between Model and Empirical 'Many' Posterior") +
#   ylab("Model's Expected Theta as a Percentile of Empirical Prior")
# with(expected_thetas, cor.test(KL, percentile))
```